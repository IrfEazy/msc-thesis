{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "30185a2fc989699c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Manual Experiments\n",
   "id": "d7279e978182b8d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## X Dataset",
   "id": "3cac1b92a82c73d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filter and import `X.json`",
   "id": "92d96d1b6691190c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T16:02:16.498003Z",
     "start_time": "2024-11-25T16:02:16.409857Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filter_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mfilter_dataset\u001B[49m(input_file\u001B[38;5;241m=\u001B[39mX_JSON, pattern\u001B[38;5;241m=\u001B[39mRETWEET_PATTERN, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'filter_dataset' is not defined"
     ]
    }
   ],
   "execution_count": 13,
   "source": [
    "\n",
    "filter_dataset(input_file=X_JSON, pattern=RETWEET_PATTERN, inplace=True)\n"
   ],
   "id": "83d1d55fd01ca6cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "save_dataset_to_csv(input_file=X_JSON, output_file=X_CSV)\n",
   "id": "f73ed2b9dc91bcb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = pd.read_csv(filepath_or_buffer=X_CSV)\n",
    "x\n"
   ],
   "id": "e8f9ae8a6f8981d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import NaÃ¯ve Bayes model and Predict binary label",
   "id": "3b46707c6c7641c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts=x['body'])\n",
    "\n",
    "gnb_dict = joblib.load(filename='models/optimal_gnb.pkl')\n",
    "gnb = gnb_dict['model']\n"
   ],
   "id": "1363c549a6849b5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x_embedding_matrix = embedding_for_vocab(\n",
    "    filepath=GLOVE_50D_MODEL,\n",
    "    word_index=tokenizer.word_index,\n",
    "    embedding_dim=EMBEDDING_50D_DIM\n",
    ")\n",
    "\n",
    "x_embedding = embed_texts(\n",
    "    texts=x['body'],\n",
    "    tokenizer=tokenizer,\n",
    "    embedding_matrix=x_embedding_matrix\n",
    ")\n",
    "\n",
    "x['predicted_labels'] = gnb.predict(X=x_embedding)\n"
   ],
   "id": "e13b5694ff5a23f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "counts = Counter(x['predicted_labels'])\n",
    "print(dict(counts))\n"
   ],
   "id": "534521b212c1d29c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reddit Dataset",
   "id": "f1e0efdc0b2e2951"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filter and import `X+Reddit.json`",
   "id": "8570e1e4caab44c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "filter_dataset(input_file=X_REDDIT_JSON, pattern=RETWEET_PATTERN, inplace=True)\n",
   "id": "54ef484d1f68a61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "save_dataset_to_csv(input_file=X_REDDIT_JSON, output_file=X_REDDIT_CSV)\n",
   "id": "768b8d24c08aad16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x_reddit = pd.read_csv(filepath_or_buffer=X_REDDIT_CSV)\n",
    "x_reddit\n"
   ],
   "id": "c9737a4352bc1bab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import NaÃ¯ve Bayes model and Predict binary label",
   "id": "6e14f12fd9f49cd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_reddit['body'])\n",
    "\n",
    "gnb_dict = joblib.load('models/optimal_gnb.pkl')\n",
    "gnb = gnb_dict['model']\n"
   ],
   "id": "5e0ad46634458549"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x_reddit_embedding_matrix = embedding_for_vocab(\n",
    "    filepath=GLOVE_50D_MODEL,\n",
    "    word_index=tokenizer.word_index,\n",
    "    embedding_dim=EMBEDDING_50D_DIM\n",
    ")\n",
    "\n",
    "x_reddit_embedding = embed_texts(\n",
    "    texts=x_reddit['body'],\n",
    "    tokenizer=tokenizer,\n",
    "    embedding_matrix=x_reddit_embedding_matrix\n",
    ")\n",
    "\n",
    "x_reddit['predicted_labels'] = gnb.predict(X=x_reddit_embedding)\n"
   ],
   "id": "1640258b0f8cb8da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "counts = Counter(x_reddit['predicted_labels'])\n",
    "print(dict(counts))\n"
   ],
   "id": "b654f6ccfc70b566"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "s_list = [\n",
    "    'Quantum computers could imperil the security of confidential electronic information, such as emails. To counter this threat, NIST has finalized its set of three encryption algorithms designed to withstand a future quantum computerâ€™s cyberattacks: https://nist.gov/news-events/news/2024/08/nist-releases-first-3-finalized-post-quantum-encryption-standards',\n",
    "    'Adversaries can deliberately manipulate AI systems to make them malfunction â€” and thereâ€™s no foolproof defense. \\n\\nCheck out NISTâ€™s new guide to the types of attacks developers and users can expect, along with approaches to mitigate them: https://nist.gov/news-events/news/2024/01/nist-identifies-types-cyberattacks-manipulate-behavior-ai-systems',\n",
    "    'In my new blog for @_CPResearch_\\n I propose a new injection technique, using the Thread Name API - check it out! ðŸ’™',\n",
    "    ' Russian Hacker Extradited to US for Phobos Ransomware Charges ',\n",
    "    ' Critical Windows Kerberos Flaw Exposes Millions of Servers to Attack - PATCH NOW ',\n",
    "    'Phishing emails increasingly use SVG attachments to evade detection',\n",
    "    'A cautionary tale from Graham Cluley at Tripwire about a New Hampshire man who has pleaded guilty to charges after having successfully tricked staff of his past employer â€“ Motorola â€“ to provide him with their login credentials to help him with a â€œtask awaiting approval.â€ His phishing emails got them to click on a link to a site that asked them to provide this information. He also sent an SMS message to a Motorola employee asking for, and receiving, their MFA code. After his arrest he attempted to order a false passport, even writing to New Hampshire Senator Maggie Hassan asking that his application be expedited. That stunt might add 10 years to the potential 20 years for the Motorola hack.'\n",
    "]\n"
   ],
   "id": "c42317819badad3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "predicted_labels = gnb.predict(X=embed_texts(\n",
    "    texts=s_list,\n",
    "    tokenizer=tokenizer,\n",
    "    embedding_matrix=embedding_for_vocab(\n",
    "        filepath=GLOVE_50D_MODEL,\n",
    "        word_index=tokenizer.word_index,\n",
    "        embedding_dim=EMBEDDING_50D_DIM\n",
    "    )\n",
    "))\n",
    "\n",
    "predicted_labels\n"
   ],
   "id": "f4ecb3a04b7edb49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example usage:\n",
    "text = \"I built this website (free, no ads or anything) and I am desperate for some feedback... :-)\\n\\nhttps://computerprogramming.art/\"\n",
    "\n",
    "# Testing the function\n",
    "print(gnb.predict(X=embed_texts(\n",
    "    texts=[process_text_with_links(text)],\n",
    "    tokenizer=tokenizer,\n",
    "    embedding_matrix=embedding_for_vocab(\n",
    "        filepath=GLOVE_50D_MODEL,\n",
    "        word_index=tokenizer.word_index,\n",
    "        embedding_dim=EMBEDDING_50D_DIM\n",
    "    )\n",
    ")))"
   ],
   "id": "552c46eb31cb1b23"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Keyword Classification Anastasia Dataset",
   "id": "be250f5b4b6898bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Anastasia Preprocessing\n",
   "id": "ca58a2577966f219"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "models = {}\n",
   "id": "34b217abe0ec3e1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "anastasia = pd.read_csv(filepath_or_buffer=ANASTASIA_CSV)\n",
    "anastasia = anastasia[anastasia['lang'] == 'en']\n",
    "anastasia.info()\n"
   ],
   "id": "8c96d028d4cbdc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a Rake instance\n",
    "rake = Rake()\n",
    "\n",
    "if Path.exists(ANASTASIA_KEYWORDS_CSV):\n",
    "    anastasia_with_keywords = pd.read_csv(filepath_or_buffer=ANASTASIA_KEYWORDS_CSV)\n",
    "else:\n",
    "    anastasia_with_keywords = pd.DataFrame(columns=['text', 'target'])\n",
    "\n",
    "    # Text from which keywords will be extracted\n",
    "    for text, label in zip(anastasia['full_text'], anastasia['Related']):\n",
    "        # Extract keywords from the text\n",
    "        rake.extract_keywords_from_text(text=process_text_with_links(text))\n",
    "\n",
    "        # Get the ranked keywords\n",
    "        keywords = rake.get_ranked_phrases_with_scores()\n",
    "        for _, keyword in keywords[:2]:\n",
    "            temp = pd.DataFrame({'text': keyword, 'target': label}, index=[0])\n",
    "            anastasia_with_keywords = pd.concat([anastasia_with_keywords, temp], ignore_index=True)\n",
    "\n",
    "    anastasia_with_keywords.to_csv(path_or_buf=ANASTASIA_KEYWORDS_CSV, index=False)\n",
    "\n",
    "anastasia_with_keywords.info()\n"
   ],
   "id": "f75a476772810b2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training NaÃ¯ve Bayes\n",
   "id": "4b6e536f83d9596b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(anastasia_with_keywords['text'])\n",
    "\n",
    "model_dict = gaussian_naive_bayes_trainer(\n",
    "    model_path=GLOVE_50D_MODEL,\n",
    "    tokenizer=tokenizer,\n",
    "    embedding_dim=50,\n",
    "    texts=anastasia_with_keywords['text'],\n",
    "    target=anastasia_with_keywords['target'],\n",
    "    cv=NUM_FOLDS,\n",
    "    random_state=RANDOM_SEED,\n",
    "    test_size=0.4\n",
    ")\n",
    "\n",
    "models['NaÃ¯ve Bayes 50D'] = model_dict\n"
   ],
   "id": "8e62b68f76f09946"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_dict = gaussian_naive_bayes_trainer(\n",
    "    model_path=GLOVE_100D_MODEL,\n",
    "    tokenizer=tokenizer,\n",
    "    embedding_dim=100,\n",
    "    texts=anastasia_with_keywords['text'],\n",
    "    target=anastasia_with_keywords['target'],\n",
    "    cv=NUM_FOLDS,\n",
    "    random_state=RANDOM_SEED,\n",
    "    test_size=0.4\n",
    ")\n",
    "\n",
    "models['NaÃ¯ve Bayes 100D'] = model_dict\n"
   ],
   "id": "40c29b5d3a618e64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_dict = gaussian_naive_bayes_trainer(\n",
    "    model_path=GLOVE_200D_MODEL,\n",
    "    tokenizer=tokenizer,\n",
    "    embedding_dim=200,\n",
    "    texts=anastasia_with_keywords['text'],\n",
    "    target=anastasia_with_keywords['target'],\n",
    "    cv=NUM_FOLDS,\n",
    "    random_state=RANDOM_SEED,\n",
    "    test_size=0.4\n",
    ")\n",
    "\n",
    "models['NaÃ¯ve Bayes 200D'] = model_dict\n"
   ],
   "id": "e892659167be1a54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_dict = gaussian_naive_bayes_trainer(\n",
    "    model_path=GLOVE_300D_MODEL,\n",
    "    tokenizer=tokenizer,\n",
    "    embedding_dim=300,\n",
    "    texts=anastasia_with_keywords['text'],\n",
    "    target=anastasia_with_keywords['target'],\n",
    "    cv=NUM_FOLDS,\n",
    "    random_state=RANDOM_SEED,\n",
    "    test_size=0.4\n",
    ")\n",
    "\n",
    "models['NaÃ¯ve Bayes 300D'] = model_dict\n"
   ],
   "id": "990a9c384816a877"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation\n",
   "id": "7c40b911c657683"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "for name, model_dict in models.items():\n",
    "    plt.plot(\n",
    "        model_dict['fpr'], model_dict['tpr'],\n",
    "        label=f\"{name}: {model_dict['roc_auc']:.2f}\"\n",
    "    )\n",
    "\n",
    "plt.title('ROC-AUC Curve')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess: 0.50')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "87efc1b8fb17b771"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "max_auc = 0.\n",
    "optimal_gnb = ''\n",
    "\n",
    "for name, model in models.items():\n",
    "    if models[name]['roc_auc'] > max_auc:\n",
    "        max_auc = models[name]['roc_auc']\n",
    "        optimal_gnb = model\n",
    "\n",
    "joblib.dump(optimal_gnb, GNB_KEYWORDS_MODEL, compress=9)\n",
    "del optimal_gnb\n",
    "\n",
    "model = joblib.load(GNB_KEYWORDS_MODEL)\n",
    "model\n"
   ],
   "id": "554b89182cf66473"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Predict X Dataset",
   "id": "ecb2c27d4eb22944"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a Rake instance\n",
    "rake = Rake()\n",
    "\n",
    "if Path.exists(X_KEYWORDS_CSV):\n",
    "    x_with_keywords = pd.read_csv(filepath_or_buffer=X_KEYWORDS_CSV)\n",
    "else:\n",
    "    x_with_keywords = pd.DataFrame(columns=['text'])\n",
    "\n",
    "    # Text from which keywords will be extracted\n",
    "    for text in x['body']:\n",
    "        # Extract keywords from the text\n",
    "        rake.extract_keywords_from_text(text=process_text_with_links(text))\n",
    "\n",
    "        # Get the ranked keywords\n",
    "        keywords = rake.get_ranked_phrases_with_scores()\n",
    "        for _, keyword in keywords[:2]:\n",
    "            temp = pd.DataFrame({'text': keyword}, index=[0])\n",
    "            x_with_keywords = pd.concat([x_with_keywords, temp], ignore_index=True)\n",
    "\n",
    "    x_with_keywords.to_csv(path_or_buf=X_KEYWORDS_CSV, index=False)\n",
    "\n",
    "x_with_keywords.info()\n"
   ],
   "id": "5ca99b93ca640d71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = pd.read_csv(filepath_or_buffer=X_CSV)\n",
    "x = x[['title', 'body']]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "gnb = model['model']\n",
    "\n",
    "x_embedding_matrix = embedding_for_vocab(\n",
    "    filepath=GLOVE_50D_MODEL,\n",
    "    word_index=tokenizer.word_index,\n",
    "    embedding_dim=EMBEDDING_50D_DIM\n",
    ")\n",
    "\n",
    "x_embedding = embed_texts(\n",
    "    texts=x['body'],\n",
    "    tokenizer=tokenizer,\n",
    "    embedding_matrix=x_embedding_matrix\n",
    ")\n",
    "\n",
    "x['predicted_labels'] = gnb.predict(X=x_embedding)\n",
    "\n",
    "counts = Counter(x['predicted_labels'])\n",
    "print(dict(counts))\n"
   ],
   "id": "31b7a6e1993e6da7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x['body'].apply(func=process_text_with_links)\n",
    "x\n"
   ],
   "id": "437cf84bd173ef25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Experimenting with Custom NER",
   "id": "9f43e757ef8b9edf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "anastasia_df = pd.read_csv(filepath_or_buffer='data/anastasia.csv')\n",
    "anastasia_df\n"
   ],
   "id": "9407bbc2770af362"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "sentence = anastasia_df['full_text'][0]\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for token in doc.ents:\n",
    "    print(f'Text:\\t{token.text}\\nStart Position:\\t{token.start_char}\\nLabel:\\t{token.label_}\\n')\n"
   ],
   "id": "1e4db8999b839541"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
