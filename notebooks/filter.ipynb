{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eacc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q hyperopt matplotlib nltk numpy pandas python-dotenv scikit-learn sentence-transformers tqdm xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a05c54365845ab7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T18:58:20.680204Z",
     "start_time": "2025-03-02T18:58:15.156839Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from typing import Any, Dict, Optional, Union\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from hyperopt import STATUS_OK, hp, Trials, fmin, tpe\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import average, ndarray\n",
    "from pandas import read_csv, DataFrame\n",
    "from sklearn import clone\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.contrib.itertools import product\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca0ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.chdir(os.getenv(\"ROOT\"))\n",
    "\n",
    "CV = int(os.getenv(\"CV\"))\n",
    "RETRAIN = True\n",
    "OVERWRITE = False\n",
    "RANDOM_STATE = int(os.getenv(\"RANDOM_STATE\"))\n",
    "TEST_SIZE = float(os.getenv(\"TEST_SIZE\"))\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee59810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.utils import preprocess_texts, replace_text_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4153e09572b632e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T18:58:21.453353Z",
     "start_time": "2025-03-02T18:58:21.445271Z"
    }
   },
   "outputs": [],
   "source": [
    "CLASSIFIERS = {\n",
    "    \"lr\": LogisticRegression(solver=\"liblinear\", max_iter=1000, random_state=RANDOM_STATE),\n",
    "    \"gnb\": GaussianNB(),\n",
    "    \"rf\": RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    \"xgb\": XGBClassifier(random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "EMBEDDING_SOURCES = [\n",
    "    {'name': 'GloVe.6B.50D', 'model-path': os.getenv(\"GLOVE_6B_50D_PATH\"), 'embedding-dim': 50},\n",
    "    {'name': 'GloVe.6B.100D', 'model-path': os.getenv(\"GLOVE_6B_100D_PATH\"), 'embedding-dim': 100},\n",
    "    {'name': 'GloVe.6B.200D', 'model-path': os.getenv(\"GLOVE_6B_200D_PATH\"), 'embedding-dim': 200},\n",
    "    {'name': 'GloVe.6B.300D', 'model-path': os.getenv(\"GLOVE_6B_300D_PATH\"), 'embedding-dim': 300},\n",
    "    {'name': 'DistilRoBERTa', 'model-path': os.getenv(\"DISTILROBERTA_PATH\"), 'embedding-dim': None},\n",
    "    {'name': 'SBERT', 'model-path': os.getenv(\"SBERT_PATH\"), 'embedding-dim': None},\n",
    "    {\"name\": \"ATT&CK-BERT\", \"model-path\": os.getenv(\"ATTACK_BERT_PATH\"), \"embedding-dim\": None},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf9147bc15c73f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T20:23:17.011679Z",
     "start_time": "2025-03-02T20:23:16.995626Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_classifier(\n",
    "        x: ndarray, y: ndarray, cv: int = 1, random_state: Optional[int] = None, test_size: float = 0.2,\n",
    "        base_estimator: Optional[ClassifierMixin] = None\n",
    ") -> Dict[str, Union[float, Any]]:\n",
    "    classifier_dict = {}\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('estimator', clone(base_estimator))\n",
    "    ])\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        shuffle=True,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    classifier_dict['cv-accuracy'] = average(a=cross_val_score(\n",
    "        estimator=pipeline,\n",
    "        X=x,\n",
    "        y=y,\n",
    "        scoring='balanced_accuracy',\n",
    "        cv=cv\n",
    "    ))\n",
    "\n",
    "    pipeline.fit(X=x_train, y=y_train)\n",
    "    y_predicted_test = pipeline.predict(X=x_test)\n",
    "    y_predicted_train = pipeline.predict(X=x_train)\n",
    "    fpr_train, tpr_train, _ = roc_curve(y_true=y_train, y_score=pipeline.predict_proba(x_train)[:, 1])\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_true=y_test, y_score=pipeline.predict_proba(x_test)[:, 1])\n",
    "\n",
    "    classifier_dict['model'] = pipeline\n",
    "\n",
    "    classifier_dict['train'] = {\n",
    "        'accuracy': balanced_accuracy_score(y_true=y_train, y_pred=y_predicted_train),\n",
    "        'precision': precision_score(y_true=y_train, y_pred=y_predicted_train),\n",
    "        'recall': recall_score(y_true=y_train, y_pred=y_predicted_train),\n",
    "        'f1': f1_score(y_true=y_train, y_pred=y_predicted_train),\n",
    "        'fpr': fpr_train,\n",
    "        'tpr': tpr_train,\n",
    "        'auc': auc(x=fpr_train, y=tpr_train)\n",
    "    }\n",
    "\n",
    "    classifier_dict['test'] = {\n",
    "        'accuracy': balanced_accuracy_score(y_true=y_test, y_pred=y_predicted_test),\n",
    "        'precision': precision_score(y_true=y_test, y_pred=y_predicted_test),\n",
    "        'recall': recall_score(y_true=y_test, y_pred=y_predicted_test),\n",
    "        'f1': f1_score(y_true=y_test, y_pred=y_predicted_test),\n",
    "        'fpr': fpr_test,\n",
    "        'tpr': tpr_test,\n",
    "        'auc': auc(x=fpr_test, y=tpr_test)\n",
    "    }\n",
    "\n",
    "    return classifier_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.getenv(\"FILTER_TRAIN_CSV\"), 'rb') as f:\n",
    "    tweets = pd.read_csv(f)\n",
    "\n",
    "texts = tweets['text']\n",
    "texts = pd.Series([replace_text_components(t) for t in texts])\n",
    "target = tweets['relevant']\n",
    "\n",
    "models_dict = {}\n",
    "\n",
    "for (embedding_info, classifier_name) in product(EMBEDDING_SOURCES, CLASSIFIERS, desc=\"Training models\", unit=\"model\"):\n",
    "    x = preprocess_texts(\n",
    "        list_str=texts,\n",
    "        model_path=embedding_info['model-path'],\n",
    "        embedding_dim=embedding_info['embedding-dim']\n",
    "    )\n",
    "\n",
    "    model_name = f\"{classifier_name.upper()} {embedding_info['name']}\"\n",
    "\n",
    "    model_dict = train_classifier(\n",
    "        x=x,\n",
    "        y=target,\n",
    "        cv=CV,\n",
    "        random_state=RANDOM_STATE,\n",
    "        test_size=TEST_SIZE,\n",
    "        base_estimator=CLASSIFIERS[classifier_name]\n",
    "    )\n",
    "\n",
    "    model_dict['name'] = model_name\n",
    "\n",
    "    try:\n",
    "        if model_dict['cv-accuracy'] > models_dict[model_name]['cv-accuracy']:\n",
    "            models_dict[model_name] = model_dict\n",
    "    except KeyError:\n",
    "        models_dict[model_name] = model_dict\n",
    "\n",
    "    print(\n",
    "        f\"Â· {classifier_name.upper()} {embedding_info['name']} - CV Accuracy:\\t{models_dict[model_name]['cv-accuracy'] * 100:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10725c0596028b64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T20:42:43.081580Z",
     "start_time": "2025-02-28T20:42:42.419937Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "for name, model_dict in models_dict.items():\n",
    "    plt.plot(\n",
    "        model_dict['test']['fpr'], model_dict['test']['tpr'],\n",
    "        label=f\"{name}: {model_dict['test']['auc']:.2f}\"\n",
    "    )\n",
    "\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fbecada005d6cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T20:45:19.699857Z",
     "start_time": "2025-02-28T20:45:19.687417Z"
    }
   },
   "outputs": [],
   "source": [
    "optimal_model = models_dict[list(models_dict.keys())[np.argmax([v[\"cv-accuracy\"] for v in models_dict.values()])]]\n",
    "\n",
    "optimal_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f06d39374b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getenv(\"FILTER_MODELS\"), 'wb') as f:\n",
    "    pickle.dump(models_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97d7b8570cfceb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T20:21:20.002667Z",
     "start_time": "2025-03-02T20:21:19.968407Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.getenv(\"FILTER_MODELS\"), 'rb') as f:\n",
    "    models_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6ad39e6b019df0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T18:58:26.508584Z",
     "start_time": "2025-03-02T18:58:26.330072Z"
    }
   },
   "outputs": [],
   "source": [
    "performances = DataFrame()\n",
    "\n",
    "for i, name in zip(DataFrame(data=models_dict.values(), index=models_dict.keys())['train'], models_dict.keys()):\n",
    "    performances[name] = DataFrame(data=i.values(), index=[c.upper() for c in i.keys()])\n",
    "\n",
    "performances = performances.T.drop(labels=['FPR', 'TPR'], axis=1)\n",
    "performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f68c6fd0a8977a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T18:58:27.000909Z",
     "start_time": "2025-03-02T18:58:26.978361Z"
    }
   },
   "outputs": [],
   "source": [
    "performances = DataFrame()\n",
    "\n",
    "for i, name in zip(DataFrame(data=models_dict.values(), index=models_dict.keys())['test'], models_dict.keys()):\n",
    "    performances[name] = DataFrame(data=i.values(), index=[c.upper() for c in i.keys()])\n",
    "\n",
    "performances = performances.T.drop(labels=['FPR', 'TPR'], axis=1)\n",
    "performances[\"CV Accuracy\"] = pd.Series({k: v[\"cv-accuracy\"] for k, v in models_dict.items()})\n",
    "performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10312cafacb54e11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T18:58:28.340379Z",
     "start_time": "2025-03-02T18:58:28.305548Z"
    }
   },
   "outputs": [],
   "source": [
    "# Round all values to 4 decimal places\n",
    "performances = performances * 100\n",
    "\n",
    "for idx, row in performances.iterrows():\n",
    "    for col in performances.columns:\n",
    "        performances.at[idx, col] = round(row[col], 2)\n",
    "\n",
    "performances.to_csv(os.getenv(\"FILTERS_PERFORMANCES_CSV\"), header=True, sep='&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142b4c9b7f246e23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-02T18:58:29.477655Z",
     "start_time": "2025-03-02T18:58:28.992032Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.getenv(\"FILTER_TEST_CSV\"), 'rb') as f:\n",
    "    test_tweets = pd.read_csv(f)\n",
    "\n",
    "texts = test_tweets['text']\n",
    "texts = pd.Series([replace_text_components(t) for t in texts])\n",
    "target = test_tweets['relevant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8952cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    preprocess_texts(\n",
    "        list_str=texts,\n",
    "        model_path='sentence-transformers/all-mpnet-base-v2',\n",
    "        embedding_dim=None,\n",
    "    ), target,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    shuffle=True,\n",
    "    stratify=target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd67a7aeec69788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    \"\"\"\n",
    "    Objective function for hyperopt to minimize the loss of a xgb model with given parameters over a dataset with a 5-fold cross-validation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : dict\n",
    "        Dictionary containing the parameters for the xgboost model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing the loss and status of the objective function.\n",
    "    \"\"\"\n",
    "    clf = XGBClassifier(\n",
    "        n_estimators=int(params['n_estimators']),\n",
    "        max_depth=int(params['max_depth']),\n",
    "        learning_rate=params['learning_rate'],\n",
    "        gamma=params['gamma'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        objective='binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    score = cross_val_score(\n",
    "        estimator=clf,\n",
    "        X=x_train,\n",
    "        y=y_train,\n",
    "        scoring='balanced_accuracy',\n",
    "        cv=2\n",
    "    ).mean()\n",
    "    return {\n",
    "        'loss': -score,\n",
    "        'status': STATUS_OK\n",
    "    }\n",
    "\n",
    "\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 1000, 1),\n",
    "    'max_depth': hp.choice('max_depth', [0, 10, 20, 30, 40, 50]),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
    "    'gamma': hp.uniform('gamma', 0.0, 0.5),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0)\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials,\n",
    "    rstate=np.random.default_rng(seed=RANDOM_STATE)\n",
    ")\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a3e32ee7f45fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'n_estimators': int(best['n_estimators']),\n",
    "    'max_depth': None if best['max_depth'] == 0 else [None, 10, 20, 30, 40, 50][best['max_depth']],\n",
    "    'min_samples_split': int(best['min_samples_split']),\n",
    "    'min_samples_leaf': int(best['min_samples_leaf']),\n",
    "    'max_features': ['sqrt', 'log2', None][best['max_features']],\n",
    "    'bootstrap': [True, False][best['bootstrap']]\n",
    "}\n",
    "\n",
    "final_clf = RandomForestClassifier(**best_params, n_jobs=-1, random_state=RANDOM_STATE)\n",
    "final_clf.fit(x_train, y_train)\n",
    "\n",
    "test_accuracy = final_clf.score(x_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbe2794017399ca",
   "metadata": {},
   "source": [
    "# Performance over Cotov's Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7da2aed20de091",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-28T20:59:44.743902Z"
    }
   },
   "outputs": [],
   "source": [
    "cotov = read_csv(filepath_or_buffer=os.getenv(\"COTOV_CSV\"))\n",
    "cotov = cotov[cotov['lang'] == 'en']\n",
    "model_name = \"RF SBERT\"\n",
    "\n",
    "cotov[f'{model_name}'] = optimal_model[\"model\"].predict(X=preprocess_texts(\n",
    "    list_str=cotov[cotov['lang'] == 'en']['full_text'],\n",
    "    model_path=os.getenv(\"SBERT_PATH\"),\n",
    "    embedding_dim=None,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4919a13732e4ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_accuracy = balanced_accuracy_score(\n",
    "    y_true=cotov['Related'],\n",
    "    y_pred=cotov[f'{model_name}']\n",
    ")\n",
    "print(f\"Accuracy of prediction over Cotov's dataset is:\\t{rf_accuracy * 100:.2f}\")\n",
    "print(classification_report(y_true=cotov['Related'], y_pred=cotov[f'{model_name}']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef96f353d9d615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERWRITE:\n",
    "    joblib.dump(final_clf, os.getenv(\"OPTIMAL_FILTER_PICKLE\"), compress=9)\n",
    "\n",
    "optimal_filter = joblib.load(os.getenv(\"OPTIMAL_FILTER_PICKLE\"))\n",
    "optimal_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c426d2547346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_filter.predict(X=preprocess_texts(\n",
    "    list_str=pd.Series(['I\\'m studying computer security', 'That\\'s a teardrop']),\n",
    "    model_path='sentence-transformers/all-mpnet-base-v2',\n",
    "    embedding_dim=None,\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
