{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f096dec80e0ebe",
   "metadata": {},
   "source": [
    "# Multi-Label Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc483734a09f5d7",
   "metadata": {},
   "source": "## Imports"
  },
  {
   "cell_type": "code",
   "id": "b428eeb33ff81277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T16:26:14.569199Z",
     "iopub.status.busy": "2024-12-12T16:26:14.568776Z",
     "iopub.status.idle": "2024-12-12T16:26:17.993533Z",
     "shell.execute_reply": "2024-12-12T16:26:17.992503Z"
    },
    "papermill": {
     "duration": 3.435545,
     "end_time": "2024-12-12T16:26:17.996431",
     "exception": false,
     "start_time": "2024-12-12T16:26:14.560886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import ast\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from numpy.typing import ArrayLike\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    hamming_loss,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from tqdm.contrib.itertools import product\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "RANDOM_STATE = int(os.getenv(\"RANDOM_STATE\"))\n",
    "TEST_SIZE = float(os.getenv(\"TEST_SIZE\"))\n",
    "\n",
    "os.chdir(os.getenv(\"ROOT\"))\n",
    "np.random.seed(seed=RANDOM_STATE)\n",
    "random.seed(a=RANDOM_STATE)\n",
    "\n",
    "import MLC\n",
    "from notebooks.utils import (\n",
    "    assess,\n",
    "    extract_models,\n",
    "    preprocess_texts,\n",
    "    replace_text_components,\n",
    "    translate_source_categories,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c146dd300f8cb589",
   "metadata": {},
   "source": [
    "class ModuleSystem(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, models, mlb):\n",
    "        self.models = models\n",
    "        self.mlb = mlb\n",
    "\n",
    "    def predict(self, X_test: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Predict labels for each component using the chosen model in the dictionary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_test : ArrayLike\n",
    "            Test data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Y_pred : ArrayLike\n",
    "            Predicted labels for each component.\n",
    "        \"\"\"\n",
    "        n_samples = X_test.shape[0]\n",
    "        n_components = len(self.models)\n",
    "        Y_pred = np.zeros((n_samples, n_components))\n",
    "\n",
    "        for idx, model_info in self.models.items():\n",
    "            model = model_info[\"model\"]\n",
    "            Y_pred[:, int(idx)] = (model.predict(X_test))[:, int(idx)]\n",
    "\n",
    "        return Y_pred"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Training",
   "id": "99d5754c059e2b92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "CLASSIFICATION_METHODS = {\n",
    "    \"br\": MLC.BRClassifier,\n",
    "    \"clr\": MLC.CLRClassifier,\n",
    "    \"cc\": MLC.CCClassifier,\n",
    "    \"lp\": MLC.LPClassifier,\n",
    "    \"pst\": MLC.PStClassifier,\n",
    "    #'cdn': MLC.CDNClassifier,\n",
    "    \"mbr\": MLC.MBRClassifier,\n",
    "    \"rakel\": MLC.RAkELClassifier,\n",
    "    \"homer\": MLC.HOMERClassifier,\n",
    "}\n",
    "\n",
    "CLASSIFICATION_ALGORITHM = {\n",
    "    \"lr\": LogisticRegression(\n",
    "        solver=\"liblinear\", max_iter=10000, random_state=RANDOM_STATE\n",
    "    ),\n",
    "    \"gnb\": GaussianNB(),\n",
    "    #\"dt\": DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    \"rf\": RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "}\n",
    "\n",
    "EMBEDDING_SOURCES = [\n",
    "    {\n",
    "        \"name\": \"GloVe.6B.50D\",\n",
    "        \"model-path\": os.getenv(\"GLOVE_6B_50D_PATH\"),\n",
    "        \"embedding-dim\": 50,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"GloVe.6B.100D\",\n",
    "        \"model-path\": os.getenv(\"GLOVE_6B_100D_PATH\"),\n",
    "        \"embedding-dim\": 100,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"GloVe.6B.200D\",\n",
    "        \"model-path\": os.getenv(\"GLOVE_6B_200D_PATH\"),\n",
    "        \"embedding-dim\": 200,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"GloVe.6B.300D\",\n",
    "        \"model-path\": os.getenv(\"GLOVE_6B_300D_PATH\"),\n",
    "        \"embedding-dim\": 300,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DistilRoBERTa\",\n",
    "        \"model-path\": os.getenv(\"DISTILROBERTA_PATH\"),\n",
    "        \"embedding-dim\": None,\n",
    "    },\n",
    "    {\"name\": \"SBERT\", \"model-path\": os.getenv(\"SBERT_PATH\"), \"embedding-dim\": None},\n",
    "    {\n",
    "        \"name\": \"ATT&CK-BERT\",\n",
    "        \"model-path\": os.getenv(\"ATTACK_BERT_PATH\"),\n",
    "        \"embedding-dim\": None,\n",
    "    },\n",
    "]\n",
    "\n",
    "print()\n",
    "SYSTEM_MODULES = [\n",
    "    {\n",
    "        \"level\": \"general\",\n",
    "        \"targets\": {\n",
    "            \"computer security\": \"computer security\",\n",
    "            \"operating systems\": \"operating systems\",\n",
    "            \"software\": \"software\",\n",
    "            \"programming languages\": \"software\",\n",
    "            \"hardware\": \"hardware\",\n",
    "            \"electronic components\": \"hardware\",\n",
    "            \"networking\": \"networking\",\n",
    "            \"internet technology\": \"networking\",\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"level\": \"computer security\",\n",
    "        \"targets\": {\n",
    "            \"network security\": \"network security\",\n",
    "            \"antivirus and malware\": \"antivirus and malware\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"level\": \"operating systems\",\n",
    "        \"targets\": {\n",
    "            \"mac os\": \"mac os\",\n",
    "            \"windows\": \"windows\",\n",
    "            \"unix\": \"unix\",\n",
    "            \"linux\": \"linux\",\n",
    "        },\n",
    "    },\n",
    "    {\"level\": \"software\", \"targets\": {\"databases\": \"databases\"}},\n",
    "    {\n",
    "        \"level\": \"hardware\",\n",
    "        \"targets\": {\n",
    "            \"computer\": \"computer\",\n",
    "            \"computer components\": \"computer components\",\n",
    "            \"computer networking\": \"computer networking\",\n",
    "        },\n",
    "    },\n",
    "]"
   ],
   "id": "1be69a703b4e8515",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(os.getenv(\"TEMP_CATEGORIES_TRAIN_CSV\"), \"rb\") as f:\n",
    "    train = pd.read_csv(f)\n",
    "    train = train.assign(\n",
    "        tweet=lambda df: df[\"tweet\"].apply(func=ast.literal_eval),\n",
    "        watson=lambda df: df[\"watson\"].apply(func=ast.literal_eval),\n",
    "        urls=lambda df: df[\"urls\"].apply(func=ast.literal_eval),\n",
    "        watson_list=lambda df: df[\"watson_list\"].apply(func=ast.literal_eval),\n",
    "        target=lambda df: df[\"target\"].apply(func=ast.literal_eval),\n",
    "    )\n",
    "    train[\"text\"] = [replace_text_components(t) for t in train[\"text\"]]\n",
    "    for idx, t in enumerate(train['relevant']):\n",
    "        if not t and 'other' not in train.loc[idx, \"target\"]:\n",
    "            train.loc[idx, \"target\"] = [\"other\"]\n",
    "\n",
    "best_module = None\n",
    "for module_config in tqdm(SYSTEM_MODULES, desc=\"Modules\", unit=\"module\"):\n",
    "    accuracy = 0\n",
    "    best_module = None\n",
    "\n",
    "    for embedder in tqdm(EMBEDDING_SOURCES, desc=\"Embedders\", unit=\"embedder\"):\n",
    "        tweets = train.assign(\n",
    "            target=lambda df: df[\"target\"].apply(\n",
    "                func=translate_source_categories, args=(module_config[\"targets\"],)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if module_config[\"level\"] != \"general\":\n",
    "            tweets = tweets[\n",
    "                tweets[\"watson_list\"].apply(lambda x: module_config[\"level\"] in x)\n",
    "            ].reset_index(drop=True)\n",
    "\n",
    "        tweets[\"text\"] = [replace_text_components(t) for t in tweets[\"text\"]]\n",
    "        texts = tweets[\"text\"]\n",
    "        x = preprocess_texts(\n",
    "            list_str=texts,\n",
    "            model_path=embedder[\"model-path\"],\n",
    "            embedding_dim=embedder[\"embedding-dim\"],\n",
    "        )\n",
    "\n",
    "        targets = tweets[\"target\"]\n",
    "        mlb = preprocessing.MultiLabelBinarizer()\n",
    "        y = mlb.fit_transform(y=targets)\n",
    "        _, y_mcp = np.unique(ar=y, axis=0, return_inverse=True)\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            x,\n",
    "            y,\n",
    "            test_size=TEST_SIZE,\n",
    "            random_state=RANDOM_STATE,\n",
    "            shuffle=True,\n",
    "            stratify=y_mcp,\n",
    "        )\n",
    "\n",
    "        models = {}\n",
    "        trained_models = []\n",
    "        for method_name, algorithm_name in product(\n",
    "                CLASSIFICATION_METHODS,\n",
    "                CLASSIFICATION_ALGORITHM,\n",
    "                desc=\"Methods & Algorithms\",\n",
    "                unit=\"pair\",\n",
    "        ):\n",
    "            model = CLASSIFICATION_METHODS[method_name](\n",
    "                CLASSIFICATION_ALGORITHM[algorithm_name]\n",
    "            )\n",
    "            try:\n",
    "                model.fit(X_train, y_train)\n",
    "                model_name = f\"{method_name.upper()}[{algorithm_name.upper()}] - {embedder['name']}\"\n",
    "            except Exception as e:\n",
    "                print(f\"{model} failed to fit due to {e}\")\n",
    "                model_name = None\n",
    "                continue\n",
    "            if model_name is not None:\n",
    "                trained_models.append(model_name)\n",
    "                models[model_name] = {\n",
    "                    \"model\": model,\n",
    "                    \"assess\": model.evaluate(X_val, y_val),\n",
    "                }\n",
    "\n",
    "        top_performing_models = extract_models(mlb.classes_, models, \"f1-score\")\n",
    "        module = ModuleSystem(top_performing_models, mlb)\n",
    "        y_val_pred = module.predict(X_val)\n",
    "        validation_results = assess(y_val, y_val_pred)\n",
    "\n",
    "        # Convert the predicted labels back to the original format using MultiLabelBinarizer\n",
    "        if validation_results[\"accuracy\"] > accuracy:\n",
    "            accuracy = validation_results[\"accuracy\"]\n",
    "            best_module = [m[\"name\"] for m in top_performing_models.values()]\n",
    "            print(\"==========================================================\")\n",
    "            print(f\"{module_config[\"level\"].capitalize()} Performance\")\n",
    "            print(\"==========================================================\")\n",
    "            print(f\"Accuracy:\\t{validation_results['accuracy'] * 100:.2f}%\")\n",
    "            print(f\"Hamming Loss:\\t{validation_results['hamming_loss']:.3f}\")\n",
    "            report = pd.DataFrame(validation_results[\"report\"])\n",
    "            report.columns = list(module.mlb.classes_) + [\n",
    "                \"micro avg\",\n",
    "                \"macro avg\",\n",
    "                \"weighted avg\",\n",
    "                \"samples avg\",\n",
    "            ]\n",
    "            report = (\n",
    "                report.transpose()\n",
    "                .map(lambda x: f\"{x:.2f}\" if isinstance(x, float) else x)\n",
    "                .to_string()\n",
    "            )\n",
    "            print(f\"{report}\")\n",
    "            print(\"==========================================================\\n\")\n",
    "\n",
    "        model_dictionary_filename = f\"{os.getenv(\"MAPPINGS_DIR\")}/{module_config[\"level\"].capitalize()}.{embedder['name']}\"\n",
    "        with open(f\"{model_dictionary_filename}.pickle\", \"wb\") as f:\n",
    "            pickle.dump(models, f)\n",
    "\n",
    "        del tweets, texts, x, targets, mlb, y, y_mcp, X_train, X_val, y_train, y_val, models, trained_models, top_performing_models, module, y_val_pred, validation_results, model_dictionary_filename\n",
    "\n",
    "    module_filename = f\"{os.getenv(\"MODELS_DIR\")}/{''.join(module_config[\"level\"].capitalize().split(' '))}\"\n",
    "    with open(f\"{module_filename}.pickle\", \"wb\") as f:\n",
    "        pickle.dump(best_module, f)"
   ],
   "id": "7eec5a65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "module_idx = 0\n",
    "with open(f\"{os.getenv(\"MAPPINGS_DIR\")}/General.ATT&CK-BERT.pickle\", \"rb\") as f:\n",
    "    general_models = pickle.load(f)\n",
    "\n",
    "with open(os.getenv(\"TEMP_CATEGORIES_TRAIN_CSV\"), \"rb\") as f:\n",
    "    train = pd.read_csv(f)\n",
    "    train = train.assign(\n",
    "        tweet=lambda df: df[\"tweet\"].apply(func=ast.literal_eval),\n",
    "        watson=lambda df: df[\"watson\"].apply(func=ast.literal_eval),\n",
    "        urls=lambda df: df[\"urls\"].apply(func=ast.literal_eval),\n",
    "        watson_list=lambda df: df[\"watson_list\"].apply(func=ast.literal_eval),\n",
    "        target=lambda df: df[\"target\"].apply(func=ast.literal_eval),\n",
    "    )\n",
    "    train[\"text\"] = [replace_text_components(t) for t in train[\"text\"]]\n",
    "    for idx, t in enumerate(train['relevant']):\n",
    "        if not t and 'other' not in train.loc[idx, \"target\"]:\n",
    "            train.loc[idx, \"target\"] = [\"other\"]\n",
    "\n",
    "tweets = train.assign(\n",
    "    target=lambda df: df[\"target\"].apply(\n",
    "        func=translate_source_categories, args=(SYSTEM_MODULES[module_idx][\"targets\"],)\n",
    "    )\n",
    ")\n",
    "\n",
    "if SYSTEM_MODULES[module_idx][\"level\"] != \"general\":\n",
    "    tweets = tweets[\n",
    "        tweets[\"watson_list\"].apply(lambda x: SYSTEM_MODULES[module_idx][\"level\"] in x)\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "targets = tweets[\"target\"]\n",
    "mlb = preprocessing.MultiLabelBinarizer()\n",
    "mlb.fit(y=targets)\n",
    "general_models = extract_models(mlb.classes_, general_models, \"f1-score\")\n",
    "module = ModuleSystem(general_models, mlb)\n",
    "\n",
    "module_filename = f\"{os.getenv(\"MODELS_DIR\")}/{''.join(SYSTEM_MODULES[module_idx][\"level\"].capitalize().split(' '))}\"\n",
    "with open(f\"{module_filename}.pickle\", \"wb\") as f:\n",
    "    pickle.dump(module, f)\n",
    "\n",
    "del tweets, targets, mlb, general_models, train, module_filename, module"
   ],
   "id": "bca90956af3b7090",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1df7d58be389a97",
   "metadata": {},
   "source": [
    "# 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "739a600adf39b195",
   "metadata": {},
   "source": [
    "with open(os.getenv(\"TEMP_CATEGORIES_TEST_CSV\"), \"rb\") as f:\n",
    "    test_tweets = pd.read_csv(f)\n",
    "    test_tweets = test_tweets.assign(\n",
    "        tweet=lambda df: df[\"tweet\"].apply(func=ast.literal_eval),\n",
    "        watson=lambda df: df[\"watson\"].apply(func=ast.literal_eval),\n",
    "        urls=lambda df: df[\"urls\"].apply(func=ast.literal_eval),\n",
    "        watson_list=lambda df: df[\"watson_list\"].apply(func=ast.literal_eval),\n",
    "        target=lambda df: df[\"target\"].apply(func=ast.literal_eval),\n",
    "    )\n",
    "    test_tweets['text'] = [replace_text_components(t) for t in test_tweets['text']]\n",
    "    relevant = test_tweets['relevant'] == True\n",
    "    is_other = test_tweets['target'].apply(lambda x: 'other' in x)\n",
    "    for idx in test_tweets[relevant & is_other].index:\n",
    "        test_tweets.loc[idx, 'relevant'] = False\n",
    "\n",
    "texts = test_tweets['text']\n",
    "targets = test_tweets['target']\n",
    "print(f\"Threat Tweets: {len(test_tweets)}\")\n",
    "test_tweets.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d916a92a504d4b6e",
   "metadata": {},
   "source": [
    "with open(\"models/General.pickle\", \"rb\") as f:\n",
    "    general_mod = pickle.load(f)\n",
    "\n",
    "with open(\"models/Computersecurity.pickle\", \"rb\") as f:\n",
    "    cs_mod = pickle.load(f)\n",
    "\n",
    "with open(\"models/Operatingsystems.pickle\", \"rb\") as f:\n",
    "    os_mod = pickle.load(f)\n",
    "\n",
    "with open(\"models/Software.pickle\", \"rb\") as f:\n",
    "    sw_mod = pickle.load(f)\n",
    "\n",
    "with open(\"models/Hardware.pickle\", \"rb\") as f:\n",
    "    hw_mod = pickle.load(f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9347e8cd01b453a7",
   "metadata": {},
   "source": [
    "class System(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, general, cs, os, sw, hw):\n",
    "        self.general = general  # ATT&CK-BERT\n",
    "        self.cs = cs  # DistilRoBERTa\n",
    "        self.os = os  # ATT&CK-BERT\n",
    "        self.sw = sw  # DistilRoBERTa\n",
    "        self.hw = hw  # ATT&CK-BERT\n",
    "\n",
    "    def predict(self, texts: pd.Series) -> ArrayLike:\n",
    "        general_dim = len(self.general.mlb.classes_)\n",
    "        cs_dim = len(self.cs.mlb.classes_) - 1\n",
    "        os_dim = len(self.os.mlb.classes_) - 1\n",
    "        sw_dim = len(self.sw.mlb.classes_) - 1\n",
    "        hw_dim = len(self.hw.mlb.classes_) - 1\n",
    "        Y_pred = np.zeros((texts.shape[0], general_dim + cs_dim + os_dim + sw_dim + hw_dim))\n",
    "        X_ATTACK_BERT = preprocess_texts(\n",
    "            list_str=texts,\n",
    "            model_path=os.getenv(\"ATTACK_BERT_PATH\"),\n",
    "            embedding_dim=None,\n",
    "        )\n",
    "        X_DistilRoBERTa = preprocess_texts(\n",
    "            list_str=texts,\n",
    "            model_path=os.getenv(\"DISTILROBERTA_PATH\"),\n",
    "            embedding_dim=None,\n",
    "        )\n",
    "        y_general = self.general.predict(X_ATTACK_BERT)\n",
    "        Y_pred[:, :general_dim] = y_general\n",
    "        for idx, t in enumerate(self.general.mlb.inverse_transform(y_general)):\n",
    "            if 'other' in t:\n",
    "                Y_pred[idx, :] = np.zeros((1, general_dim + cs_dim + os_dim + sw_dim + hw_dim))\n",
    "                Y_pred[idx, 4:5] = 1.0\n",
    "            if 'computer security' in t and 'other' not in t:\n",
    "                y_cs = self.cs.predict(X_DistilRoBERTa[idx:idx + 1])\n",
    "                Y_pred[idx, general_dim:general_dim + cs_dim] = y_cs[:, :cs_dim]\n",
    "            if 'operating systems' in t and 'other' not in t:\n",
    "                y_os = self.os.predict(X_ATTACK_BERT[idx:idx + 1])\n",
    "                Y_pred[idx, general_dim + cs_dim:general_dim + cs_dim + os_dim] = np.hstack(\n",
    "                    (y_os[:, :2], y_os[:, 3:]))\n",
    "            if \"software\" in t and 'other' not in t:\n",
    "                y_sw = self.sw.predict(X_DistilRoBERTa[idx:idx + 1])\n",
    "                Y_pred[idx, general_dim + cs_dim + os_dim:general_dim + cs_dim + os_dim + sw_dim] = y_sw[:, :sw_dim]\n",
    "            if \"hardware\" in t and 'other' not in t:\n",
    "                y_hw = self.hw.predict(X_ATTACK_BERT[idx:idx + 1])\n",
    "                Y_pred[idx, general_dim + cs_dim + os_dim + sw_dim:] = y_hw[:, :hw_dim]\n",
    "            if len(t) == 0:\n",
    "                Y_pred[idx, 4:5] = 1.0\n",
    "        return Y_pred\n",
    "\n",
    "    def predict_targets(self, texts: pd.Series) -> pd.Series:\n",
    "        general_dim = len(self.general.mlb.classes_)\n",
    "        cs_dim = len(self.cs.mlb.classes_) - 1\n",
    "        os_dim = len(self.os.mlb.classes_) - 1\n",
    "        sw_dim = len(self.sw.mlb.classes_) - 1\n",
    "        Y_pred = self.predict(texts)\n",
    "        y_general = Y_pred[:, :general_dim]\n",
    "        y_cs = Y_pred[:, general_dim:general_dim + cs_dim]\n",
    "        y_os = Y_pred[:, general_dim + cs_dim:general_dim + cs_dim + os_dim]\n",
    "        y_sw = Y_pred[:, general_dim + cs_dim + os_dim:general_dim + cs_dim + os_dim + sw_dim]\n",
    "        y_hw = Y_pred[:, general_dim + cs_dim + os_dim + sw_dim:]\n",
    "        targets_general = self.general.mlb.inverse_transform(y_general)\n",
    "        targets_cs = self.cs.mlb.inverse_transform(np.hstack((y_cs, np.zeros((y_cs.shape[0], 1)))))\n",
    "        targets_os = self.os.mlb.inverse_transform(np.hstack((y_os[:, :2], np.zeros((y_os.shape[0], 1)), y_os[:, 2:])))\n",
    "        targets_sw = self.sw.mlb.inverse_transform(np.hstack((y_sw, np.zeros((y_sw.shape[0], 1)))))\n",
    "        targets_hw = self.hw.mlb.inverse_transform(np.hstack((y_hw, np.zeros((y_hw.shape[0], 1)))))\n",
    "        return pd.Series(data=[list(set(target_general + target_cs + target_os + target_sw + target_hw)) for\n",
    "                               target_general, target_cs, target_os, target_sw, target_hw in\n",
    "                               zip(targets_general, targets_cs, targets_os, targets_sw, targets_hw)], index=texts.index)\n",
    "\n",
    "    def evaluate(self, texts: pd.Series, targets: pd.Series) -> dict[str, float]:\n",
    "        targets_general = self.general.mlb.transform(targets)\n",
    "        targets_cs = self.cs.mlb.transform(targets)\n",
    "        targets_os = self.os.mlb.transform(targets)\n",
    "        targets_sw = self.sw.mlb.transform(targets)\n",
    "        targets_hw = self.hw.mlb.transform(targets)\n",
    "        Y_test = np.hstack((targets_general, targets_cs[:, :-1], targets_os[:, :2], targets_os[:, 3:],\n",
    "                            targets_sw[:, :-1], targets_hw[:, :-1]))\n",
    "        Y_pred = self.predict(texts)\n",
    "        return assess(Y_test, Y_pred)\n",
    "\n",
    "    def evaluate_filter(self, texts: pd.Series, targets: pd.Series) -> dict[str, float]:\n",
    "        Y_test = pd.Series([1 if t else 0 for t in targets])\n",
    "        Y_pred = self.predict(texts)\n",
    "        return assess(Y_test, Y_pred[:, 4:5])\n",
    "\n",
    "    def classes(self):\n",
    "        return list(self.general.mlb.classes_) + list(self.cs.mlb.classes_[:-1]) + list(\n",
    "            self.os.mlb.classes_[:2]) + list(self.os.mlb.classes_[3:]) + list(self.sw.mlb.classes_[:-1]) + list(\n",
    "            self.hw.mlb.classes_[:-1])\n",
    "\n",
    "\n",
    "solution = System(general_mod, cs_mod, os_mod, sw_mod, hw_mod)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pd.DataFrame(\n",
    "    [\n",
    "        solution.predict_targets(texts[:10]), targets[:10], texts[:10],\n",
    "        test_tweets['relevant'][:10]\n",
    "    ],\n",
    "    index=[\"Predicted\", \"Actual\", \"Text\", \"Relevant\"]\n",
    ").transpose()"
   ],
   "id": "f2e62cbfaca42c1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fa2cddd383fc303f",
   "metadata": {},
   "source": [
    "system_performance = solution.evaluate(texts, targets)\n",
    "print(\"==========================================================\")\n",
    "print(\"System Performance\")\n",
    "print(\"==========================================================\")\n",
    "print(f\"Accuracy:\\t{system_performance['accuracy'] * 100:.2f}%\")\n",
    "print(f\"Hamming Loss:\\t{system_performance['hamming_loss']:.3f}\")\n",
    "report = pd.DataFrame(system_performance[\"report\"])\n",
    "report.columns = solution.classes() + [\"micro avg\", \"macro avg\", \"weighted avg\", \"samples avg\"]\n",
    "report = (report.transpose().map(lambda x: f\"{x:.2f}\" if isinstance(x, float) else x).to_string())\n",
    "print(f\"{report}\")\n",
    "print(\"==========================================================\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Y_test = pd.Series([1 if t else 0 for t in test_tweets['relevant']])\n",
    "Y_pred = pd.Series([not t for t in solution.predict(texts)[:, 4:5]])\n",
    "auc_score_macro = roc_auc_score(Y_test, Y_pred, average=\"macro\")\n",
    "auc_score_weighted = roc_auc_score(Y_test, Y_pred, average=\"weighted\")\n",
    "report = classification_report(Y_test, Y_pred, output_dict=True, zero_division=0.0)\n",
    "report[\"macro avg\"][\"auc\"] = auc_score_macro\n",
    "report[\"weighted avg\"][\"auc\"] = auc_score_weighted\n",
    "system_performance = {\"hamming_loss\": hamming_loss(Y_test, Y_pred), \"report\": report}\n",
    "\n",
    "print(\"==========================================================\")\n",
    "print(\"System Performance\")\n",
    "print(\"==========================================================\")\n",
    "print(f\"Hamming Loss:\\t{system_performance['hamming_loss']:.3f}\")\n",
    "report = pd.DataFrame(system_performance[\"report\"])\n",
    "report.columns = [\"not relevant\", \"relevant\"] + [\"accuracy\", \"macro avg\", \"weighted avg\"]\n",
    "report = (report.transpose().map(lambda x: f\"{x:.2f}\" if isinstance(x, float) else x).to_string())\n",
    "print(f\"{report}\")\n",
    "print(\"==========================================================\\n\")"
   ],
   "id": "ecc9fad7fadd800f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pd.DataFrame(np.hstack((Y_test.values.reshape(-1, 1), Y_pred.values.reshape(-1, 1), test_tweets[\"text\"].values.reshape(-1, 1))), columns=[\"relevant\", \"predicted\", \"text\"])",
   "id": "cbb38756b03db5e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"models/System.pickle\", \"wb\") as f:\n",
    "    pickle.dump(solution, f)\n",
    "\n",
    "del solution"
   ],
   "id": "76a8c9607e6acdde",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 870709,
     "sourceId": 1483651,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4678553,
     "sourceId": 7954760,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "vsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5394.913296,
   "end_time": "2024-12-12T17:56:06.436689",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-12T16:26:11.523393",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
