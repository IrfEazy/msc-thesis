{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports\n",
   "id": "46641496e2f377e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "import string\n",
    "from pathlib import Path\n",
    "\n",
    "import gensim.models\n",
    "import inflect\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch.cuda\n",
    "#from google.colab import drive\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Input, MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from transformers import pipeline\n"
   ],
   "id": "445c57b851cfc46a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "nltk.download('stopwords')\n",
   "id": "6031a73346b20ac0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Environment Settings\n",
   "id": "e618c919441f7bfc"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "# Define the regex pattern\n",
    "RETWEET_PATTERN = r\"RT\\ \\@.*\"\n",
    "EVERY_PATTERN = r\".*\"\n",
    "\n",
    "# CSV files\n",
    "X_CSV = Path(\"data/X.csv\")\n",
    "\n",
    "# JSON files\n",
    "THREAT_TWEETS_JSON = Path('data/threat.tweets.json')\n",
    "X_JSON = Path(\"data/X.json\")\n",
    "\n",
    "# Directories\n",
    "#ROOT_DIR = Path('./')\n",
    "#THESIS_DIR = ROOT_DIR + '/'\n",
    "\n",
    "# Models\n",
    "WORD2VEC_BIN = Path('models/GoogleNews-vectors-negative300.bin')\n",
    "WORD2VEC_BIN_GZ = Path('models/GoogleNews-vectors-negative300.bin.gz')\n",
    "WORD2VEC_BIN_GZ_URL = 'https://drive.usercontent.google.com/download?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download&authuser=0&confirm=t&uuid=7d62ae10-fee5-4471-a14b-d3fc3c8de6cf&at=AENtkXYp0oeqJDsqv8DR2sbelnZ5%3A1732188868578'\n",
    "\n",
    "# Constant variables\n",
    "EMBEDDING_50D_DIM = 50\n",
    "NUM_FOLDS = 10\n",
    "RANDOM_SEED = 42\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if os.path.exists(WORD2VEC_BIN):\n",
    "    print('File already decompressed.')\n",
    "else:\n",
    "    # Check if the compressed file exists\n",
    "    if os.path.exists(WORD2VEC_BIN_GZ):\n",
    "        try:\n",
    "            with gzip.open(WORD2VEC_BIN_GZ, 'rb') as f_in:\n",
    "                with open(WORD2VEC_BIN, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "            print('File decompressed successfully.')\n",
    "        except OSError as e:\n",
    "            print(f'An error occurred during decompression: {e}')\n",
    "    else:\n",
    "        # Download the file from Google Drive if it's not present\n",
    "        response = requests.get(WORD2VEC_BIN_GZ_URL)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            with open(WORD2VEC_BIN_GZ, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "            print('File downloaded successfully.')\n",
    "\n",
    "        try:\n",
    "            with gzip.open(WORD2VEC_BIN_GZ, 'rb') as f_in:\n",
    "                with open(WORD2VEC_BIN, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "            print('File decompressed successfully.')\n",
    "        except OSError as e:\n",
    "            print(f'An error occurred during decompression: {e}')\n"
   ],
   "id": "f35364edc75b46ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Functions\n",
   "id": "e00833295fe00d51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def replace_digits_with_words(tokens):\n",
    "    inflect_engine = inflect.engine()\n",
    "    return [inflect_engine.number_to_words(token) if token.isdigit() else token for token in tokens]\n"
   ],
   "id": "95ab455b72a52943",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# [A Framework for Unsupervised Classification and Data Mining of Tweets about Cyber Vulnerabilities](https://arxiv.org/abs/2104.11695)\n",
    "\n",
    "Recent studies have indicated that the NVD is not always up to date, with known vulnerabilities being discussed publicly on social media platforms, like Twitter and Reddit, months before they are published to the NVD. To that end, we present a framework for unsupervised classification to filter tweets for relevance to cybersecurity. We consider and evaluate two unsupervised ML techniques for inclusion in our framework, and show that zero-shot classification using a Bidirectional and Auto-Regressive Transformers (BART) model outperforms the other technique with 83.52% accuracy and a F1 score of 83.88%, allowing for accurate filtering of tweets without human intervention or labelled data for training.\n",
    "\n",
    "Additionally, we discuss different insights that can be derived from these cyber-relevant tweets, such as trending topics of tweets and the counts of Twitter mentions for Common Vulnerabilities and Exposures (CVEs), that can be used in an alert or report to augment current NVD-based risk assessment tools.\n"
   ],
   "id": "717f778564fb331b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this experiment, the unsupervised techniques were evaluated on a labeled dataset of tweets from Behzadan et al.\n",
   "id": "1a244950ded1df53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## [Corpus and Deep Learning Classifier for Collection of Cyber Threat Indicators in Twitter Stream](https://ieeexplore.ieee.org/document/8622506)\n",
    "\n",
    "A corpus of 21.000 tweets was curated directly from [Twitter](https://github.com/behzadanksu/cybertweets)\n"
   ],
   "id": "cc67dd9b4cd565bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load JSON into a pandas DataFrame\n",
    "threat_tweets = pd.read_json(path_or_buf=THREAT_TWEETS_JSON)\n"
   ],
   "id": "e54239dc6b6baea3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "threat_tweets['preprocess-text'] = threat_tweets['text']\n",
    "\n",
    "# 1. Conversion of all characters of the tweet to lower case\n",
    "threat_tweets['preprocess-text'] = threat_tweets['preprocess-text'].str.lower()\n",
    "\n",
    "# 2. Tokenize the text according to white-space separations\n",
    "threat_tweets['preprocess-text'] = threat_tweets['preprocess-text'].str.split()\n",
    "\n",
    "# 3. Remove tokens that are not encoded in ASCII\n",
    "threat_tweets['preprocess-text'] = threat_tweets['preprocess-text'].apply(\n",
    "    lambda tokens: [token for token in tokens if token.isascii()]\n",
    ")\n",
    "\n",
    "# 4. Remove punctuation from each token\n",
    "threat_tweets['preprocess-text'] = threat_tweets['preprocess-text'].apply(\n",
    "    lambda tokens: [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
    ")\n",
    "\n",
    "# 5. Remove tokens that are not composed of alphanumeric characters\n",
    "threat_tweets['preprocess-text'] = threat_tweets['preprocess-text'].apply(\n",
    "    lambda tokens: [token for token in tokens if token.isalnum()]\n",
    ")\n",
    "\n",
    "# 6. Substitute digits with word representations (e.g., 4 -> four)\n",
    "threat_tweets['preprocess-text'] = threat_tweets['preprocess-text'].apply(replace_digits_with_words)\n",
    "\n",
    "# 7. Remove stop words\n",
    "threat_tweets['preprocess-text'] = threat_tweets['preprocess-text'].apply(\n",
    "    lambda tokens: [word for word in tokens if word.lower() not in stop_words]\n",
    ")\n",
    "\n",
    "# 8. Stem tokens\n",
    "threat_tweets['preprocess-text'] = threat_tweets['preprocess-text'].apply(\n",
    "    lambda tokens: [stemmer.stem(token) for token in tokens]\n",
    ")\n"
   ],
   "id": "42d70c103811f3f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "threat_tweets.info()\n",
   "id": "7148397b8ae28501",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Instantiate the model\n",
    "model = Sequential(layers=[\n",
    "    Input(shape=(300, 1)),\n",
    "    Conv1D(filters=32, kernel_size=8, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(30, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'recall', 'precision', 'f1_score']\n",
    ")\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ],
   "id": "fcbbefa7404e9c8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sentence_to_embedding(sentence, model, embedding_dim):\n",
    "    # Initialize a list to store the embeddings for the words in the sentence\n",
    "    word_embeddings = []\n",
    "\n",
    "    for word in sentence:\n",
    "        # Check if the word exists in the model's vocabulary\n",
    "        if word in model.key_to_index:\n",
    "            word_embeddings.append(model[word])\n",
    "        else:\n",
    "            # If the word is not in the vocabulary, use a zero vector\n",
    "            word_embeddings.append(np.zeros(embedding_dim))\n",
    "\n",
    "    # If no words are in the vocabulary, return a zero vector\n",
    "    if len(word_embeddings) == 0:\n",
    "        return np.zeros(embedding_dim)\n",
    "\n",
    "    # Average the word embeddings to get the sentence embedding\n",
    "    sentence_embedding = np.mean(word_embeddings, axis=0)\n",
    "\n",
    "    return sentence_embedding\n"
   ],
   "id": "74b4ba7ea2ffe37c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embedder = gensim.models.KeyedVectors.load_word2vec_format(WORD2VEC_BIN, binary=True)\n",
    "\n",
    "X = np.array([\n",
    "    sentence_to_embedding(words, embedder, embedding_dim=300)\n",
    "    for words in threat_tweets['preprocess-text']\n",
    "])\n",
    "y = np.array([1. if yi == True else 0. for yi in threat_tweets['relevant']])\n"
   ],
   "id": "c5a761390ca0b308",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_val, y_val,\n",
    "    test_size=0.5,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y_val\n",
    ")\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "\n",
    "y_val = y_val.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights_dict\n",
    ").history\n"
   ],
   "id": "14a0316160e492ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display the training history\n",
    "plt.plot(history['accuracy'], label='accuracy')\n",
    "plt.plot(history['val_accuracy'], label='val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ],
   "id": "a49d2deada59fdbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy, recall, precision, f1 = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ],
   "id": "b171af91d093c7f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Continue\n",
   "id": "24a112e6f574ce43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "values = \"\\n\".join([f\"\\t· {str(s)}\" for s in threat_tweets[\"annotation\"].unique()])\n",
    "\n",
    "print(f'The original values in the field \"annotation\":\\n{values}')\n"
   ],
   "id": "bc485d5e909f9139",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The dataset contains 21.368 tweets collected over four days using common cybersecurity keywords, and were labeled as 'threat,' 'business,' 'unknown,' and 'irrelevant.' Since we focused on vulnerabilities, we first filtered the dataset for tweets that contain the term 'vulnerability,' which came out to 9.963 tweets.\n",
   "id": "2fce7adf6f40c5ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Filter rows where 'text' column contains the word 'vulnerability'\n",
    "vulnerability_threat_tweets = threat_tweets[\n",
    "    threat_tweets['type'].apply(lambda x: any('vulnerability' in t.lower() for t in x))]\n",
    "vulnerability_threat_tweets = vulnerability_threat_tweets.dropna(subset=['annotation'])\n",
    "\n",
    "values = \"\\n\".join([f\"\\t· {str(s)}\" for s in vulnerability_threat_tweets[\"annotation\"].unique()])\n",
    "\n",
    "print(f'The original values in the field \"annotation\":\\n{values}\\n')\n",
    "vulnerability_threat_tweets.info()\n"
   ],
   "id": "fe1e3326feac6910",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tweets labeled as ‘business,’ ‘unknown,’ and ‘threat’ were replaced with a ‘cyber-relevant’ label as they also appeared to be relevant to cybersecurity, and comprised 54.5% of the filtered dataset.",
   "id": "f8898231da896bc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "vulnerability_threat_tweets['annotation'].value_counts()\n",
   "id": "72ecbf9f134076ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vulnerability_threat_tweets.loc[\n",
    "    vulnerability_threat_tweets['annotation'].isin(['business', 'unknown', 'threat']), 'y-test'] = 1\n",
    "vulnerability_threat_tweets.loc[vulnerability_threat_tweets['annotation'].isin(['irrelevant']), 'y-test'] = 0\n",
    "vulnerability_threat_tweets['y-test'] = vulnerability_threat_tweets['y-test'].astype(int)\n",
    "\n",
    "vulnerability_threat_tweets['y-test'].value_counts()\n"
   ],
   "id": "1c82b43bc26a9b2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "classifier = pipeline(\n",
    "    task=\"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "candidate_labels = [\n",
    "    \"The text describes a personal or emotional vulnerability, unrelated to technology or cybersecurity.\",\n",
    "    \"The text describes a cybersecurity-related vulnerability, such as a weakness in software, systems, or networks.\"\n",
    "]\n",
    "\n",
    "candidate_labels_dict = {label: i for i, label in enumerate(candidate_labels)}\n"
   ],
   "id": "1dfd321466c56a01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vulnerability_threat_tweets['zero-shot-bart-large-mnli'] = [\n",
    "    classifier(sequence_to_classify, candidate_labels)['labels'][0]\n",
    "    for sequence_to_classify in vulnerability_threat_tweets['text']\n",
    "]\n",
    "\n",
    "vulnerability_threat_tweets['zero-shot-bart-large-mnli'] = vulnerability_threat_tweets['zero-shot-bart-large-mnli'].map(\n",
    "    candidate_labels_dict\n",
    ")\n"
   ],
   "id": "d1cb5e991108dae5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute the accuracy and the F1 score of the model by knowing that the column y-test has the correct values, while zero-shot-bart-large-mnli has the predicted labels\n",
    "\n",
    "accuracy = accuracy_score(\n",
    "    vulnerability_threat_tweets['y-test'],\n",
    "    vulnerability_threat_tweets['zero-shot-bart-large-mnli']\n",
    ")\n",
    "\n",
    "f1 = f1_score(\n",
    "    vulnerability_threat_tweets['y-test'],\n",
    "    vulnerability_threat_tweets['zero-shot-bart-large-mnli']\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ],
   "id": "9296306233666873",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "vulnerability_threat_tweets[['zero-shot-bart-large-mnli', 'y-test', 'text']]\n",
   "id": "bda3755f7d4fdf66",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
